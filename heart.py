# -*- coding: utf-8 -*-
"""heart.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15kRn4n8Db_HYti-7GhH4M4lhb805tda2
"""

# prompt: read csv "/content/sample_data/heart.csv"  df.head

import pandas as pd
df = pd.read_csv("/content/sample_data/heart.csv")
df.head()

# Realizar un análisis estadístico descriptivo
descriptive_stats = df.describe()

# Calcular el número de valores únicos para cada variable categórica
unique_values = df[['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']].nunique()

descriptive_stats, unique_values

import seaborn as sns
import matplotlib.pyplot as plt

# Configuración del estilo de los gráficos
sns.set(style="whitegrid")

# Correlaciones entre las variables numéricas
corr = df.corr()

# Configuración del tamaño del gráfico de la matriz de correlación
plt.figure(figsize=(12, 10))

# Crear un mapa de calor para visualizar las correlaciones
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Mapa de Calor de Correlaciones entre Variables Numéricas")
plt.show()

# Ejemplos de agrupación de datos según categorías y comparación de estadísticas

# Agrupar por Sexo y calcular estadísticas descriptivas para variables numéricas
grouped_by_sex = df.groupby('sex').agg(['mean', 'std'])

# Agrupar por Tipo de Dolor de Pecho (cp) y calcular estadísticas descriptivas para variables numéricas
grouped_by_cp = df.groupby('cp').agg(['mean', 'std'])

# Mostrar las estadísticas agrupadas por Sexo y Tipo de Dolor de Pecho
grouped_by_sex, grouped_by_cp

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Binarización de la variable objetivo
df['target_bin'] = (df['target'] >= 0.5).astype(int)

# Separar las características y la variable objetivo
X = df.drop(['target', 'target_bin'], axis=1)
y = df['target_bin']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Estandarización de las características
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Entrenar el modelo de regresión logística
model = LogisticRegression()
model.fit(X_train, y_train)

# Predicciones y evaluación del modelo
y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

report, conf_matrix

# Configurar visualización más presentable para el reporte de clasificación y la matriz de confusión

# Crear un DataFrame para el reporte de clasificación
report_df = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose()

# Configurar el tamaño del gráfico de la matriz de confusión
plt.figure(figsize=(8, 6))

# Crear un mapa de calor para la matriz de confusión
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", square=True)
plt.title("Matriz de Confusión")
plt.xlabel("Predicciones")
plt.ylabel("Valores Reales")
plt.xticks(ticks=[0.5, 1.5], labels=["Bajo Riesgo (0)", "Alto Riesgo (1)"])
plt.yticks(ticks=[0.5, 1.5], labels=["Bajo Riesgo (0)", "Alto Riesgo (1)"])
plt.show()

report_df

